{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup Google Colab by running this cell only once (ignore this if run locally) {display-mode: \"form\"}\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/epfl-exts/aiml2days.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"aiml2days/notebooks/data\" \"aiml2days/notebooks/data_prep_tools.py\" \"aiml2days/notebooks/EDA_tools.py\" \"aiml2days/notebooks/modeling_tools.py\" . \n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"aiml2days/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "# Data\n",
    "\n",
    "We will use the [SpamAssassin](https://spamassassin.apache.org/) public email corpus. This dataset contains ~6'000 labeled emails. If you want to learn more about this dataset, check [this](https://spamassassin.apache.org/old/publiccorpus/). (*Note: Datasets of text are called corpora and samples are called documents.*) \n",
    "\n",
    "The dataset has been downloaded for you and is available in the *data* folder.\n",
    "\n",
    "The dataset has been labelled, i.e. we are told whether an email has been designated as spam, .e.g. if it was flagged by a user, or whether it is considered an example of regular emails (non-spam, also called \"ham\"). \n",
    "\n",
    "Our goal is to explore and compare various features space and machine learning approaches. The use of spam emails is just for demonstration and learning purpose as it is a text-based example that everyone is easily familiar with and that allows us to highlight different stages of developing a machine learning application and the decision making processes involved along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data preparation :: Overview\n",
    "\n",
    "In this notebook we will explore the dataset, do a first analysis and prepare it for different machine learning tasks.\n",
    "\n",
    "### Task \n",
    "\n",
    "We will process the raw data, clean the text and extract additional features ain order to prepare it for further analysis and for building our machine learning models.\n",
    "\n",
    "### Notebook overview\n",
    "\n",
    "* Load the data\n",
    "* Text preprocessing\n",
    "* Feature extraction\n",
    "* Store cleaned data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and helper functions\n",
    "%run data_prep_tools.py\n",
    "%run EDA_tools.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8546 emails loaded\n",
      "Cleaning data set:\n",
      "2710 duplicate emails found and removed\n",
      "4 empty emails found and removed\n",
      "\n",
      "5832 emails remaining\n",
      "\n",
      "Number of columns: 2\n",
      "Columns names:\n",
      "spam_label, text\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "df_source = load_source_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "spam_label",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "5bba24bf-77bc-456c-8827-a04c3052c92e",
       "rows": [
        [
         "5426",
         "0",
         "RAH quoted: >Indians are not poor because there are too many of them; they are poor >because there are too many regulations and too much government intervention >-- even today, a decade after reforms were begun. India's greatest problems >arise from a political culture guided by socialist instincts on the one >hand and an imbedded legal obligation on the other hand. Nice theory and all, but s/India/France/g and the statements hold just as true, yet France is #12 in the UN's HDI ranking, not #124. >Since all parties must stand for socialism, no party espouses >classical liberalism I'm not convinced that that classical liberalism is a good solution for countries in real difficulty. See Joseph Stiglitz (Nobel for Economics) on the FMI's failed remedies. Of course googling on \"Stiglitz FMI\" only brings up links in Spanish and French. I guess that variety of spin is non grata in many anglo circles. R http://xent.com/mailman/listinfo/fork"
        ],
        [
         "2918",
         "0",
         "Update on this for anyone that's interested, and because I like closed threads... nothing worse than an infinite while loop, is there? I ended up formatting a floppy on my flatmate's (un-networked) P100 running FAT16 Win95, and mcopied the contents of the bootdisk across. Now I have a FAT16 Win98 install running alongside Slackware, and can play Metal Gear Solid when the mood takes me ;) /Ciaran. On Wednesday 21 August 2002 16:21, Ciaran Johnston wrote: > Dublin said: > > If you copy the files from your disk to the c: partition and mark it as > > active it should work ... > > Yeah, I figured that, but it doesn't seem to ... well, if that's the case > I'll give it another go tonight, maybe come back with some error messages. > > Just to clarify for those who didn't understand me initially - I have a > floppy drive installed, but it doesn't physically work. There's nowhere > handy to pick one up where I am, and I don't fancy waiting a few days for > one to arrive from Peats. > > Thanks for the answers, > Ciaran. > > > You especially need io.sys, command.com and msdos.sys > > > > your cd driver .sys and read the autoexec.bat and config.sys files for > > hints on what you did with your boot floppy <g> > > > > P > > > > On Wed, 2002-08-21 at 14:07, Ciaran Johnston wrote: > >> Hi folks, > >> The situation is this: at home, I have a PC with 2 10Gig HDDs, and no > >> (working) floppy drive. I have been running Linux solely for the last > >> year, but recently got the urge to, among other things, play some of > >> my Windoze games. I normally install the windows partition using a > >> boot floppy which I have conveniently zipped up, but I haven't any way > >> of writing or reading a floppy. > >> So, how do I go about: > >> 1. formatting a C: drive with system files (normally I would use > >> format /s c: from the floppy). > >> 2. Installing the CDROM drivers (my bootdisk (I wrote it many years > >> ago) does this normally). > >> 3. Booting from the partition? > >> > >> I wiped all my linux partitions from the first drive and created > >> partitions for Windows (HDA1) Slackware and RedHat. I used cfdisk for > >> this. I made the first drive (hda) bootable. I then installed the > >> windows partition in LILO and reran lilo (installed in MBR). I copied > >> the contents of boot.zip to my new windows partition and tried to boot > >> it - all I get is a garbled line of squiggles. > >> > >> Anyone any ideas? I can't think of anywhere in Athlone to get a new > >> floppy drive this evening... > >> > >> Thanks, > >> Ciaran. > >> > >> > >> > >> -- > >> Irish Linux Users' Group: ilug@linux.ie > >> http://www.linux.ie/mailman/listinfo/ilug for (un)subscription > >> information. List maintainer: listmaster@linux.ie -- Irish Linux Users' Group: ilug@linux.ie http://www.linux.ie/mailman/listinfo/ilug for (un)subscription information. List maintainer: listmaster@linux.ie"
        ],
        [
         "5312",
         "0",
         "This is the Postfix program at host kci.kciLink.com. #################################################################### # THIS IS A WARNING ONLY. YOU DO NOT NEED TO RESEND YOUR MESSAGE. # #################################################################### Your message could not be delivered for 4.0 hours. It will be retried until it is 5.0 days old. For further assistance, please send mail to <postmaster> The Postfix program <khera@kcilink.com>: connect to yertle.kcilink.com[216.194.193.105]: Operation timed out"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam_label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5426</th>\n",
       "      <td>0</td>\n",
       "      <td>RAH quoted: &gt;Indians are not poor because there are too many of them; they are poor &gt;because there are too many regulations and too much government intervention &gt;-- even today, a decade after reforms were begun. India's greatest problems &gt;arise from a political culture guided by socialist instincts on the one &gt;hand and an imbedded legal obligation on the other hand. Nice theory and all, but s/India/France/g and the statements hold just as true, yet France is #12 in the UN's HDI ranking, not #124. &gt;Since all parties must stand for socialism, no party espouses &gt;classical liberalism I'm not convinced that that classical liberalism is a good solution for countries in real difficulty. See Joseph Stiglitz (Nobel for Economics) on the FMI's failed remedies. Of course googling on \"Stiglitz FMI\" only brings up links in Spanish and French. I guess that variety of spin is non grata in many anglo circles. R http://xent.com/mailman/listinfo/fork</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2918</th>\n",
       "      <td>0</td>\n",
       "      <td>Update on this for anyone that's interested, and because I like closed threads... nothing worse than an infinite while loop, is there? I ended up formatting a floppy on my flatmate's (un-networked) P100 running FAT16 Win95, and mcopied the contents of the bootdisk across. Now I have a FAT16 Win98 install running alongside Slackware, and can play Metal Gear Solid when the mood takes me ;) /Ciaran. On Wednesday 21 August 2002 16:21, Ciaran Johnston wrote: &gt; Dublin said: &gt; &gt; If you copy the files from your disk to the c: partition and mark it as &gt; &gt; active it should work ... &gt; &gt; Yeah, I figured that, but it doesn't seem to ... well, if that's the case &gt; I'll give it another go tonight, maybe come back with some error messages. &gt; &gt; Just to clarify for those who didn't understand me initially - I have a &gt; floppy drive installed, but it doesn't physically work. There's nowhere &gt; handy to pick one up where I am, and I don't fancy waiting a few days for &gt; one to arrive from Peats. &gt; &gt; Thanks for the answers, &gt; Ciaran. &gt; &gt; &gt; You especially need io.sys, command.com and msdos.sys &gt; &gt; &gt; &gt; your cd driver .sys and read the autoexec.bat and config.sys files for &gt; &gt; hints on what you did with your boot floppy &lt;g&gt; &gt; &gt; &gt; &gt; P &gt; &gt; &gt; &gt; On Wed, 2002-08-21 at 14:07, Ciaran Johnston wrote: &gt; &gt;&gt; Hi folks, &gt; &gt;&gt; The situation is this: at home, I have a PC with 2 10Gig HDDs, and no &gt; &gt;&gt; (working) floppy drive. I have been running Linux solely for the last &gt; &gt;&gt; year, but recently got the urge to, among other things, play some of &gt; &gt;&gt; my Windoze games. I normally install the windows partition using a &gt; &gt;&gt; boot floppy which I have conveniently zipped up, but I haven't any way &gt; &gt;&gt; of writing or reading a floppy. &gt; &gt;&gt; So, how do I go about: &gt; &gt;&gt; 1. formatting a C: drive with system files (normally I would use &gt; &gt;&gt; format /s c: from the floppy). &gt; &gt;&gt; 2. Installing the CDROM drivers (my bootdisk (I wrote it many years &gt; &gt;&gt; ago) does this normally). &gt; &gt;&gt; 3. Booting from the partition? &gt; &gt;&gt; &gt; &gt;&gt; I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5312</th>\n",
       "      <td>0</td>\n",
       "      <td>This is the Postfix program at host kci.kciLink.com. #################################################################### # THIS IS A WARNING ONLY. YOU DO NOT NEED TO RESEND YOUR MESSAGE. # #################################################################### Your message could not be delivered for 4.0 hours. It will be retried until it is 5.0 days old. For further assistance, please send mail to &lt;postmaster&gt; The Postfix program &lt;khera@kcilink.com&gt;: connect to yertle.kcilink.com[216.194.193.105]: Operation timed out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      spam_label  \\\n",
       "5426           0   \n",
       "2918           0   \n",
       "5312           0   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \n",
       "5426                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               RAH quoted: >Indians are not poor because there are too many of them; they are poor >because there are too many regulations and too much government intervention >-- even today, a decade after reforms were begun. India's greatest problems >arise from a political culture guided by socialist instincts on the one >hand and an imbedded legal obligation on the other hand. Nice theory and all, but s/India/France/g and the statements hold just as true, yet France is #12 in the UN's HDI ranking, not #124. >Since all parties must stand for socialism, no party espouses >classical liberalism I'm not convinced that that classical liberalism is a good solution for countries in real difficulty. See Joseph Stiglitz (Nobel for Economics) on the FMI's failed remedies. Of course googling on \"Stiglitz FMI\" only brings up links in Spanish and French. I guess that variety of spin is non grata in many anglo circles. R http://xent.com/mailman/listinfo/fork  \n",
       "2918  Update on this for anyone that's interested, and because I like closed threads... nothing worse than an infinite while loop, is there? I ended up formatting a floppy on my flatmate's (un-networked) P100 running FAT16 Win95, and mcopied the contents of the bootdisk across. Now I have a FAT16 Win98 install running alongside Slackware, and can play Metal Gear Solid when the mood takes me ;) /Ciaran. On Wednesday 21 August 2002 16:21, Ciaran Johnston wrote: > Dublin said: > > If you copy the files from your disk to the c: partition and mark it as > > active it should work ... > > Yeah, I figured that, but it doesn't seem to ... well, if that's the case > I'll give it another go tonight, maybe come back with some error messages. > > Just to clarify for those who didn't understand me initially - I have a > floppy drive installed, but it doesn't physically work. There's nowhere > handy to pick one up where I am, and I don't fancy waiting a few days for > one to arrive from Peats. > > Thanks for the answers, > Ciaran. > > > You especially need io.sys, command.com and msdos.sys > > > > your cd driver .sys and read the autoexec.bat and config.sys files for > > hints on what you did with your boot floppy <g> > > > > P > > > > On Wed, 2002-08-21 at 14:07, Ciaran Johnston wrote: > >> Hi folks, > >> The situation is this: at home, I have a PC with 2 10Gig HDDs, and no > >> (working) floppy drive. I have been running Linux solely for the last > >> year, but recently got the urge to, among other things, play some of > >> my Windoze games. I normally install the windows partition using a > >> boot floppy which I have conveniently zipped up, but I haven't any way > >> of writing or reading a floppy. > >> So, how do I go about: > >> 1. formatting a C: drive with system files (normally I would use > >> format /s c: from the floppy). > >> 2. Installing the CDROM drivers (my bootdisk (I wrote it many years > >> ago) does this normally). > >> 3. Booting from the partition? > >> > >> I...  \n",
       "5312                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         This is the Postfix program at host kci.kciLink.com. #################################################################### # THIS IS A WARNING ONLY. YOU DO NOT NEED TO RESEND YOUR MESSAGE. # #################################################################### Your message could not be delivered for 4.0 hours. It will be retried until it is 5.0 days old. For further assistance, please send mail to <postmaster> The Postfix program <khera@kcilink.com>: connect to yertle.kcilink.com[216.194.193.105]: Operation timed out  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you rerun this cell multiple times you get different samples displayed each time\n",
    "# OR you can replace the number 3 with a number of your choice\n",
    "display(df_source.sample(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Text preprocessing\n",
    "\n",
    "Good text preprocessing is an essential part of every NLP project. It is the first step in the machine learning pipeline and it is important to get it right. The goal of text preprocessing is to transform the raw text into a format that can be used by machine learning algorithms.\n",
    "\n",
    "Our overall goal is to build models that can help us distinguish non-spam from spam. \n",
    "\n",
    "The examples above have shown us that some samples are quite messy and contain a lot of content unnecessary for understanding the text as a human, i.e. they contain \"noise\". As a first step we will \"*clean*\" and \"*standardize*\" raw text. Our aim is to keep as many \"*informative*\" words as possible, while discarding the \"*uniformative*\" ones. Removing the noise from our texts will help to improve the accuracy of our models.\n",
    "\n",
    "We thus need to identify which parts of the text are acting as \"*noise*\" in our text and remove it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Task:\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "<h3>Questions</h3>\n",
    "    \n",
    "__Q1.__ What parts of the text do you think are noise?\n",
    "   \n",
    "__Q2.__ What should we do with these parts of the text?\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💡 Observations\n",
    "\n",
    "Observations from the discussions in the slide presentation:\n",
    "\n",
    "1. There are some items in the text that should be removed to make it readable. Here are some suggestions:\n",
    "\n",
    "* HTML tags \n",
    "* URLs\n",
    "* E-mail addresses\n",
    "* Punctuation marks, digits (e.g. 2002, 1.1, ...)\n",
    "* Multiple whitespaces\n",
    "* Case conversion (e.g. Dog vs dog, ...)\n",
    "* English STOPWORDS (e.g. a, is, my, i, all, and, by...)\n",
    "* ...\n",
    "\n",
    "2. From experience, we know that the number of occurrences of the above items (HTML tags, URLs, etc) can be helpful to distinguish spam from non-spam. Similarly, the length of the emails and the frequency of punctuation marks or upper case letters could also give us clues as to whether we are dealing with spam or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *clean_corpus* function below will take care of the parts raised in the 1st set of observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 5832\n",
      "Number of columns: 3\n",
      "Columns names:\n",
      "spam_label, text, text_cleaned\n",
      "\n",
      "Number of duplicate cleaned texts found: 279\n",
      "Number of empty texts found: 27\n",
      "\n",
      "Email texts cleaned\n",
      "Number of samples: 5832\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = clean_corpus(df_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Original document:\n",
      "\n",
      "SUMMARY: Everything you need to know about every registered US Business is on one single CD-ROM. The\n",
      "latest version of the USA Business Search CD contains a wealth of information on 18 million business\n",
      "listings in the US. So whether you're after new business, building or enhancing your own database or\n",
      "simply want cheap quality leads, USA Business Search CD will pinpoint exactly what you are looking\n",
      "for. USA Business Search is an easy to use CD-ROM compiled using the most accurate and comprehensive\n",
      "USA Yellow Pages (11.6 million records) and USA Domain Names (6.2 million records) databases. At\n",
      "just $429.00, USA Business Search CD is a cost effective way of generating UNLIMITED leads and\n",
      "spotting opportunities. Unrestricted export capability enables you to import data into your database\n",
      "application. Where else can you get this for $429.00? For the same data InfoUSA will charge you over\n",
      "$600,000.00 WHAT IS USA BUSINESS SEARCH CD? USA Business Search CD gives you unlimited access to\n",
      "search, filter, extract, and import data by the following fields: 1. USA Businesses/Yellow Pages\n",
      "database: Category, SIC Codes/Description, Company Name, Street, City, State, Zip, Phone Number. 2.\n",
      "USA Domain Names database: Domain Name, Company Name, Registrant Name, Registrant Email, Registrant\n",
      "Address, Registrant Phone, Registrant Fax. - Over 18 million listings with phone numbers - Over 6.7\n",
      "million contact names with email addresses - Over 3 million listings with fax numbers Available also\n",
      "complete listings of Canadian, European and Latin American Businesses. Please reply back with the\n",
      "Subject: MORE INFO. HOW DOES USA BUSINESS SEARCH WORK? Once you purchase USA Business Search CD for\n",
      "$429.00, all data on the CD-ROM can be viewed, printed, exported without any limitations. A built-in\n",
      "Search Engine lets you export information to create new prospect spreadsheets and databases or\n",
      "enhance your current ones. WHY USA BUSINESS SEARCH? USA Business Search CD is excellent value for\n",
      "money, accurate\n",
      "\n",
      "Cleaned document:\n",
      "\n",
      "summary need know registered business single latest version business search contains wealth\n",
      "information million business listings youre business building enhancing database simply want cheap\n",
      "quality leads business search pinpoint exactly looking business search easy compiled using accurate\n",
      "comprehensive yellow pages million records domain names million records databases just business\n",
      "search cost effective generating unlimited leads spotting opportunities unrestricted export\n",
      "capability enables import data database application data infousa charge business search business\n",
      "search gives unlimited access search filter extract import data following fields businesses yellow\n",
      "pages database category codes description company street city state phone number domain names\n",
      "database domain company registrant registrant email registrant address registrant phone registrant\n",
      "million listings phone numbers million contact names email addresses million listings numbers\n",
      "available complete listings canadian european latin american businesses reply subject info does\n",
      "business search work purchase business search data viewed printed exported limitations built search\n",
      "engine lets export information create prospect spreadsheets databases enhance current ones business\n",
      "search business search excellent value money accurate easy quite simply invaluable sales marketing\n",
      "tool companies size looking business marketing sales customer services business search save make\n",
      "money quickly identify target markets customers using variety criteria produce cost effective direct\n",
      "mailing lists instantly print directly labels addressed named contacts create telemarketing\n",
      "campaigns avoid costly calls service bureaus simply view telephone number clean enhance spreadsheets\n",
      "databases cost effective suppliers place order accepting credit card check orders place order\n",
      "complete form print form mail address order line mail datanetwork north road suite miami beach beach\n",
      "copy paste form order print sign mail send businesses dom\n"
     ]
    }
   ],
   "source": [
    "# Let's look at some examples.\n",
    "# You can rerun this cell to get a different sample\n",
    "show_clean_text(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering \n",
    "\n",
    "## Part 1: Extracting numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with the ideas from the 2nd observation and create new features that count different noise components of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples and columns of input: (5832, 2)\n",
      "Number of columns: 2\n",
      "Columns names:\n",
      "spam_label, text\n",
      "\n",
      "Numeric features extracted\n",
      "Data size: (5832, 14)\n",
      "Number of columns: 14\n",
      "Columns names:\n",
      "email_counts, html tag_counts, url_counts, Twitter username_counts, hashtag_counts\n",
      "character_counts, word_counts, unique word_counts, punctuation mark_counts, uppercase word_counts\n",
      "lowercase word_counts, digit_counts, alphabetic char_counts, spam_label\n",
      "Numeric features saved to data/num_features.csv\n"
     ]
    }
   ],
   "source": [
    "num_features_df = extract_numeric_features(df=df_source, with_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "## Part 2: Extracting features from text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computers don't understand natural language and its unstructured form. So, how do we represent text?\n",
    "\n",
    "### Bag of words\n",
    "\n",
    "One of the simplest but in the early days of NLP effective and commonly used models to represent text for machine learning is the ***Bag of Words*** model ([link](https://en.wikipedia.org/wiki/Bag-of-words_model)). When using this model, we discard most of the structure of the input text (word order, chapters, paragraphs, sentences or formatting) and only count how often each word appears in each text. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a \"bag\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Example:** Let our toy corpus contain four documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "corpus = [\n",
    "    'I enjoy paragliding.',\n",
    "    'I do like NLP.',\n",
    "    'I like deep learning.',\n",
    "    'O Captain! my Captain!'\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "captain",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "deep",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "do",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "enjoy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "i",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "learning",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "like",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "my",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "nlp",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "o",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "paragliding",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "c5d2705a-d7c9-4f0f-9043-166e7117dfa0",
       "rows": [
        [
         "I enjoy paragliding.",
         "0",
         "0",
         "0",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1"
        ],
        [
         "I do like NLP.",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0",
         "1",
         "0",
         "0"
        ],
        [
         "I like deep learning.",
         "0",
         "1",
         "0",
         "0",
         "1",
         "1",
         "1",
         "0",
         "0",
         "0",
         "0"
        ],
        [
         "O Captain! my Captain!",
         "2",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "1",
         "0",
         "1",
         "0"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>captain</th>\n",
       "      <th>deep</th>\n",
       "      <th>do</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>i</th>\n",
       "      <th>learning</th>\n",
       "      <th>like</th>\n",
       "      <th>my</th>\n",
       "      <th>nlp</th>\n",
       "      <th>o</th>\n",
       "      <th>paragliding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I enjoy paragliding.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I do like NLP.</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like deep learning.</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O Captain! my Captain!</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        captain  deep  do  enjoy  i  learning  like  my  nlp  \\\n",
       "Text                                                                           \n",
       "I enjoy paragliding.          0     0   0      1  1         0     0   0    0   \n",
       "I do like NLP.                0     0   1      0  1         0     1   0    1   \n",
       "I like deep learning.         0     1   0      0  1         1     1   0    0   \n",
       "O Captain! my Captain!        2     0   0      0  0         0     0   1    0   \n",
       "\n",
       "                        o  paragliding  \n",
       "Text                                    \n",
       "I enjoy paragliding.    0            1  \n",
       "I do like NLP.          0            0  \n",
       "I like deep learning.   0            0  \n",
       "O Captain! my Captain!  1            0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_bag_of_words_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the table above, each column represents a word from the corpus and each row one of the four documents. The value in each cell represents the number of times that word appears in a specific document. For example, the fourth document has the word `captain` occurring twice and the words `my` and `O` occurring once.\n",
    "\n",
    "The technical implementation of  Bag of Words is called a CountVectorizer. It converts each document into a rows of numbers, i.e. a numeric vector. Thus the name vectorizer.  \n",
    "\n",
    "While this kind of transformation allows machine learning algorithms to process text data effectively, it has a drawback. It treats all words as independent and ignores the context in which they appear. For example, losing information about the order of the words in the text can change the meaning of a sentence. The sentences \"I do like NLP\", \"Do I like NLP\" or \"NLP like I do\" have the same set of words but different meanings. \n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "The **Term Frequency–Inverse Document Frequency** approach aims to address this limitation, by measuring how important a word is for a document relative to a collection of documents (the corpus). \n",
    "\n",
    "We use the implementation by scikit-learn. It calculates the TF-IDF score as the product of :\n",
    "- The **term frequency TF**, which is the ratio of the frequency of the word $w$ in the given document $d$ divided by the total number of words in the given document.   \n",
    "  So $TF(w, d) = \\frac{f(w, d)}{N(d)}$\n",
    "- and the (smoothed) )**inverse document frequency IDF**, which is given by \n",
    "$$IDF(w, D) = \\log\\left(\\frac{size(D)+1}{df(w, D)+1}\\right)+1$$ \n",
    "where $df(w, D)$ is the number of documents in the corpus $D$ that contain the word $w$. Adding `1` in the numerator and denominator keeps the IDF value finite and stable.\n",
    "\n",
    "This way, common words that appear in many documents (small IDF) are given less weight while rare words that appear in only a few documents get a higher weight (high IDF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "Text",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "captain",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "deep",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "do",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "enjoy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "i",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "learning",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "like",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "my",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "nlp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "o",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "paragliding",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "1e198590-97e2-4afd-8835-b919fedd074f",
       "rows": [
        [
         "I enjoy paragliding.",
         "0.0",
         "0.0",
         "0.0",
         "0.6445029922609534",
         "0.41137791133379387",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.6445029922609534"
        ],
        [
         "I do like NLP.",
         "0.0",
         "0.0",
         "0.5745795256791941",
         "0.0",
         "0.3667466683744504",
         "0.0",
         "0.4530050977381881",
         "0.0",
         "0.5745795256791941",
         "0.0",
         "0.0"
        ],
        [
         "I like deep learning.",
         "0.0",
         "0.5745795256791941",
         "0.0",
         "0.0",
         "0.3667466683744504",
         "0.5745795256791941",
         "0.4530050977381881",
         "0.0",
         "0.0",
         "0.0",
         "0.0"
        ],
        [
         "O Captain! my Captain!",
         "0.816496580927726",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.0",
         "0.408248290463863",
         "0.0",
         "0.408248290463863",
         "0.0"
        ]
       ],
       "shape": {
        "columns": 11,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>captain</th>\n",
       "      <th>deep</th>\n",
       "      <th>do</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>i</th>\n",
       "      <th>learning</th>\n",
       "      <th>like</th>\n",
       "      <th>my</th>\n",
       "      <th>nlp</th>\n",
       "      <th>o</th>\n",
       "      <th>paragliding</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Text</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I enjoy paragliding.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.644503</td>\n",
       "      <td>0.411378</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.644503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I do like NLP.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366747</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57458</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I like deep learning.</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57458</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.366747</td>\n",
       "      <td>0.57458</td>\n",
       "      <td>0.453005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>O Captain! my Captain!</th>\n",
       "      <td>0.816497</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         captain     deep       do     enjoy         i  \\\n",
       "Text                                                                     \n",
       "I enjoy paragliding.    0.000000  0.00000  0.00000  0.644503  0.411378   \n",
       "I do like NLP.          0.000000  0.00000  0.57458  0.000000  0.366747   \n",
       "I like deep learning.   0.000000  0.57458  0.00000  0.000000  0.366747   \n",
       "O Captain! my Captain!  0.816497  0.00000  0.00000  0.000000  0.000000   \n",
       "\n",
       "                        learning      like        my      nlp         o  \\\n",
       "Text                                                                      \n",
       "I enjoy paragliding.     0.00000  0.000000  0.000000  0.00000  0.000000   \n",
       "I do like NLP.           0.00000  0.453005  0.000000  0.57458  0.000000   \n",
       "I like deep learning.    0.57458  0.453005  0.000000  0.00000  0.000000   \n",
       "O Captain! my Captain!   0.00000  0.000000  0.408248  0.00000  0.408248   \n",
       "\n",
       "                        paragliding  \n",
       "Text                                 \n",
       "I enjoy paragliding.       0.644503  \n",
       "I do like NLP.             0.000000  \n",
       "I like deep learning.      0.000000  \n",
       "O Captain! my Captain!     0.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "show_tfidf_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below you can extract the text features using either the CountVectorizer (`vectorizer=\"count\"`) or the TfidfVectorizer (`vectorizer=\"tfidf\"`). Please note that this process takes a while, so be patient.\n",
    "\n",
    "For that reason, we have already pre-computed the features using `\"tfidf\"`and stored them in the `features` folder. You can load them using the command `load_feature_space(features=\"text\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectorizer selected\n",
      "Text features saved to 'data/text_features_tfidf.csv'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5832, 10001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_features_df = extract_text_features(\n",
    "    df_cleaned, vectorizer=\"tfidf\", with_labels=True, store=True\n",
    ")\n",
    "text_features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings\n",
    "\n",
    "The Bag of Words and TF-IDF approaches cannot capture the meaning of words or the relationships between them. They also lead to very high-dimensional and sparse representations of the text which are not very efficient and can lead to overfitting.\n",
    "To address these limitations, we can use **embeddings** or transformer based models. Embeddings are denser vector representations of words are learned from large corpora of text. By representing similar words as similar vectors they can capture meaning and relationships in a continuous lower-dimensional vector space.\n",
    "\n",
    "We have passed the email texts through a language model to generate the associated embeddings. Since the feature extraction takes some time we have stored these embeddings and made them available for you in the file named `email_embeddings.csv`.\n",
    "\n",
    "You can load them using the command `load_feature_space(features=\"embeddings\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email embeddings loaded\n",
      "Data includes labels in the column 'spam_label'\n",
      "The data set has 5832 rows, 769 columns\n"
     ]
    }
   ],
   "source": [
    "embeddings_df = load_feature_space(features=\"embedding\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
