{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup Google Colab by running this cell only once (ignore this if run locally) {display-mode: \"form\"}\n",
    "import sys\n",
    "\n",
    "if 'google.colab' in sys.modules:\n",
    "    # Clone GitHub repository\n",
    "    !git clone https://github.com/epfl-exts/aiml2days.git\n",
    "        \n",
    "    # Copy files required to run the code\n",
    "    !cp -r \"aiml2days/notebooks/data\" \"aiml2days/notebooks/data_prep_tools.py\" \"aiml2days/notebooks/EDA_tools.py\" \"aiml2days/notebooks/modeling_tools.py\" . \n",
    "    \n",
    "    # Install packages via pip\n",
    "    !pip install -r \"aiml2days/colab-requirements.txt\"\n",
    "    \n",
    "    # Restart Runtime\n",
    "    import os\n",
    "    os.kill(os.getpid(), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries and helper functions\n",
    "%run data_prep_tools.py\n",
    "%run EDA_tools.py\n",
    "%run modeling_tools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Building a spam detector\n",
    "\n",
    "## In this notebook\n",
    "\n",
    "Our aim is to build a simple spam detector. These are the main steps:\n",
    "* Load a feature set into memory\n",
    "* Split the data into training and test set (see day 1)\n",
    "* Train a model\n",
    "* Evaluate the model\n",
    "* Analyze misclassified samples to gain further insights\n",
    "\n",
    "You **main task** is to rerun the modelling part several times on different settings. You can change the data, the evaluation metric and more...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "For our first run we will just use the numerical features for now. But don't worry it will be your turn to explore what happens when you **later load the other feature sets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features_df = load_feature_space(\"num\", no_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the number of samples per class in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = load_labels()\n",
    "plot_class_frequency(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "Suppose you applied a **very naive approach** to the spam detection problem **that uses none of the features**: _You just either classify all emails as \"spam\" or as \"non-spam\"._\n",
    "\n",
    "__Q1.__ How many emails would be classified correctly in each case?  \n",
    "\n",
    "__Q2.__ Which approach would be more successful?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just established a **baseline** against which we will compare all other models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model building\n",
    "\n",
    "Throughout this notebook we will use a **Logistic Regression classifier**. Here is why:\n",
    "- It is a simple and efficient model for binary classification tasks. \n",
    "- It is a good baseline for more complex models. \n",
    "- It is fast to train and thus allows us to quickly iterate on our model and try out different settings.\n",
    "- It is also easy to interpret and allows us to explore where our model makes mistakes.\n",
    "\n",
    "Below you will\n",
    "- build a first simple model.\n",
    "- tune the main hyperparameter `C` for the model using a cross-validated grid search.\n",
    "- explore different feature sets and see how they affect the performance of the model.\n",
    "- explore the effect of different evaluation metrics.\n",
    "- Explore misclassified samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first trial\n",
    "\n",
    "#### Training a single model\n",
    "\n",
    "As a first trial, we will use the `num` feature set with a simpl. The accuracy is defined as the number of correct predictions divided by the total number of predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(num_features_df)\n",
    "\n",
    "# Fit model on the train data\n",
    "model = fit_model(df_train, C=1)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "__Q1.__ Which numbers tell use the correct predictions for each class?\n",
    "\n",
    "__Q2.__ Which numbers tell use the failed predictions for each class?\n",
    "\n",
    "__Q3.__ Which class faired better?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Classification report\n",
    "\n",
    "The classification report provides us with different 4 metrics to evaluate the performance of our model: overall accuracy, precision, recall and f1-score. For a reminder expand the text cell below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "3 metrics for per class:  \n",
    "The **precision**  looks at the predictions per class. It divides the number of correct predictions by the number predictions made for that class. In the confusion matrix the emphasis is per column (vertical).\n",
    "\n",
    "The **recall** looks at the ground truth per class. It divides the number of samples in that class that were correctly predicted by the number of samples in that class (support). In the confusion matrix  the emphasis is per column (horizontal).\n",
    "\n",
    "The **f1-score** is the harmonic mean of precision and recall.\n",
    "\n",
    "Overall metric:  \n",
    "The **accuracy** is the number of all correct predictions divided by the total number of samples for the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "The column named \"Support\" gives the total number of samples for each class in the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more systematic approach:\n",
    "#### Fine tuning with grid search and cross-validation\n",
    "\n",
    "We will use a 5-fold cross-validation (`cv=5`). We also collect all the results from the cross-validation so we can plot them below. The process will automatically choose the best model for us. Finally we use the test set will to `evaluate` the performance of our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(num_features_df)\n",
    "# text_features_df takes 45mins\n",
    "\n",
    "# Fit model on the train data\n",
    "model, cv_results = fit_log_reg_model(df_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Tracking overfitting\n",
    "\n",
    "Below we plot the results of the cross-validation. For each value of `C` we want to compare the training scores (blue) against the validation score (orange). The red cross marks the value of the best `C`.\n",
    "\n",
    "In order to assess overfitting we are interested in the gap between the training and validation curves. (for details expand the text cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "If the gap is small, it means that our model is not overfitting and generalizes well to unseen data.  \n",
    "If the gap is large, it means that our model is overfitting. This indicates that the model has learned irrelevant information like noise that does not reflect the general pattern. In such a case we need to find ways to adjust the model to reduce the gap and improve the performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cv_results(cv_results, show_table=False, plot_confidence=False, plot_fit_time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "__Q1.__ Do we observe overfitting i.e. a large gap between the training and validation curves?\n",
    "\n",
    "__Q2.__ What happens when C is very small and when it is very large?\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more insights into the model\n",
    "\n",
    "The coefficients of the Logistic Regression model can tell us how much each feature contributes to the overall prediction. The larger the absolute value of a coefficient, the more important the corresponding feature is for the model. \n",
    "\n",
    "For the impact on the overall prediction we look at _feature values times coefficients_. This will help us understand the model's behavior better and identify which features are driving the predictions for particular samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coefficients(model, df_train, n_top_features=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sure was the model of its predictions?\n",
    "The Logistic Regression model provides the probabilities of each class for each sample. This allows us to assess the confidence of the model's predictions.\n",
    "\n",
    "Low probabilities (close to 0) indicate that the model is very sure that the sample is not spam. High probabilities (close to 1) indicate that the model is very sure that the sample is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_certainties(df_test, model, log_scale=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Careful with interpretation if the top plot is using a log-scale. This means that the values are not evenly spaced. You can change the setting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "\n",
    "\n",
    "<h3>Questions</h3>\n",
    "\n",
    "__Q1.__ When the model misclassified a sample, is it usually very sure of its prediction, or kind of doubtful?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis : Where does our model fail?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The *error_analysis* function below will show us the top features responsible for the model's wrong decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(df_test, model, doc_nbr=10, n_top_coeff=5, color_by_coeff_sign=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NOW IT'S YOUR TURN\n",
    "\n",
    "We have copied the above code blocks again below. You can use them to build your own spam detector now. \n",
    "\n",
    "There are a number of things you can adapt:\n",
    "\n",
    "### Change the feature space\n",
    "\n",
    "We have loaded 4 feature spaces at the start of the notebook. Simply replace `num_features_df` with `text_features`, `num_text_features`, or `embedding_features` in the code below to use a different feature space.\n",
    "\n",
    "Warning: The feature spaces using text features are quite slow (45 mins) and will take quite a while to run the fine-tuning with cross-validation.  \n",
    "The pre-computed output of grid search with cross-validation can be loaded with the following code `cv_results=pd.read_csv(\"text_log_reg_cv_results.csv\")`\n",
    "    \n",
    "You can retrain the model using the `fit_model`-function with `C` set to the best `C`-value from `cv_results`.\n",
    "    \n",
    "### Change the metric used for fine-tuning\n",
    "\n",
    "You can change the scoring function inside `fit_log_reg_model(df_train)`.  \n",
    "\n",
    "The current default value is `scoring=None` which will use the accuracy score. \n",
    "\n",
    "However, you can also change the scoring function to `\"precision\"`, `\"recall\"`, or `\"f1\"` and check how the results change. \n",
    "\n",
    "More options are given [here](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
    "\n",
    "What happens to the confusion matrix as you vary the metric?\n",
    "\n",
    "### Explore other settings\n",
    "\n",
    "You will likely have to change some of the other parameters in the visualisations, etc. to make them more interpretable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the other feature spaces\n",
    "\n",
    "### Working locally on your machine\n",
    "If you generated the text based features spaces in the notebook `data_preparation.ipynb` then you will be able to load them directly from the `data` folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the remaining feature spaces\n",
    "embeddings_df = load_feature_space(\"embedding\", no_labels=False)\n",
    "text_features_df = load_feature_space(\"text\", no_labels=False)\n",
    "num_text_features_df = load_feature_space(\"num_text\", no_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If working with Colab - else ignore\n",
    "\n",
    "If you are working on Google Colab, you will need to rerun the feature generation for text (code below) because the different notebooks don't share the same instance of the data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate text features (this can take around 1:15 min)\n",
    "df_source = load_source_data()\n",
    "df_cleaned = clean_corpus(df_source)\n",
    "\n",
    "text_features_df = extract_text_features(\n",
    "    df_cleaned, vectorizer=\"tfidf\", with_labels=True, store=True\n",
    ")\n",
    "# You can switch to vectorizer=\"count\" later if you want to try\n",
    "\n",
    "text_features_df = load_feature_space(\"text\", no_labels=False)\n",
    "num_text_features_df = load_feature_space(\"num_text\", no_labels=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Time to play\n",
    "\n",
    "#### To make things easier: Change your settings here and then run the cell below\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_space = num_features_df\n",
    "# options are: \n",
    "# \"num_features_df\"\n",
    "# \"text_features_df\"\n",
    "# \"num_text_features_df\"\n",
    "# \"embeddings_df\"\n",
    "\n",
    "\n",
    "C = 1\n",
    "# Some options to try are: 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000\n",
    "\n",
    "\n",
    "scoring = None\n",
    "# options include: 'accuracy', 'f1', 'precision', 'recall'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(feature_space)\n",
    "\n",
    "# Fit model on the train data\n",
    "model = fit_model(df_train, C=C)\n",
    "\n",
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning with grid search and cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/test splitting\n",
    "df_train, df_test = train_test_split_(feature_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note for using text features\n",
    "With text features this can take a while to run, so you can skip this first cell and load the pre-computed `cv_results`in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model on the train data\n",
    "model, cv_results = fit_log_reg_model(df_train, scoring=scoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load fine tuning results for text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For text features only\n",
    "cv_results = pd.read_csv(\"text_log_reg_cv_results.csv\")\n",
    "display(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit model with the best C on the train data\n",
    "best_C = ?\n",
    "\n",
    "model = fit_model(df_train, C=best_C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracking overfitting for all cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viz_cv_results(cv_results, show_table=False, plot_confidence=False, plot_fit_time=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print classification report for test set\n",
    "classification_report_(df_test, model)\n",
    "\n",
    "# Print predictions on test set\n",
    "plot_confusion_matrix(df_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get more insights into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_coefficients(model, df_train, n_top_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How sure was the model of its predictions?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_prediction_certainties(df_test, model, log_scale=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error analysis :: Where does our model fail?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_analysis(df_test, model, doc_nbr=10, n_top_coeff=15, color_by_coeff_sign=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking for more\n",
    "\n",
    "### Why not use Gemini  as a coding partner to explore some of your own ideas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:adsml2024]",
   "language": "python",
   "name": "conda-env-adsml2024-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
